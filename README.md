# Spotify Azure Data Engineering Project

A comprehensive data engineering solution built on Azure cloud services for processing Spotify streaming data using modern data architecture patterns.

## ğŸ—ï¸ Architecture Overview

This project implements a **Medallion Architecture** with **Incremental Data Processing** using Azure cloud services:

```
Azure SQL Database (Source) â†’ Azure Data Factory â†’ Azure Data Lake Gen2 â†’ Azure Databricks â†’ Unity Catalog
     4 Tables              Orchestration    Bronze/Silver/Gold      Processing         Governance
```

## ğŸ“Š Data Flow Architecture

### Source Data (Azure SQL Database)
- **dim_user**: User demographics and subscription details
- **dim_artist**: Artist information and metadata  
- **dim_date**: Date dimension for time-based analytics
- **fact_stream**: Streaming events and play history

### Medallion Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BRONZE LAYER  â”‚    â”‚  SILVER LAYER   â”‚    â”‚   GOLD LAYER    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ Raw Parquet     â”‚â”€â”€â”€â–¶â”‚ Cleaned Delta   â”‚â”€â”€â”€â–¶â”‚ SCD Type 2      â”‚
â”‚ Files           â”‚    â”‚ Tables          â”‚    â”‚ Analytics Ready â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Data Lake     â”‚    â”‚ â€¢ Validated     â”‚    â”‚ â€¢ Aggregated    â”‚
â”‚ â€¢ Partitioned   â”‚    â”‚ â€¢ Deduplicated  â”‚    â”‚ â€¢ Historized    â”‚
â”‚ â€¢ Compressed    â”‚    â”‚ â€¢ Enriched      â”‚    â”‚ â€¢ Optimized     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Incremental Data Pipeline

### AutoLoader Implementation
```
Source Changes â†’ AutoLoader Detection â†’ Stream Processing â†’ Delta Merge â†’ SCD Type 2 Updates
```

**Key Features:**
- **Change Data Capture**: Tracks modifications using watermarks
- **Exactly-Once Processing**: Ensures data consistency
- **Schema Evolution**: Handles structure changes automatically
- **Checkpoint Management**: Enables recovery and restart capabilities

## ğŸ› ï¸ Technology Stack

### Azure Services
- **Azure Data Factory**: Pipeline orchestration and scheduling
- **Azure Databricks**: Spark-based data processing
- **Azure Data Lake Storage Gen2**: Scalable data storage
- **Azure SQL Database**: Source system with 4 core tables
- **Azure Key Vault**: Secrets and credential management

### Processing Technologies
- **Apache Spark**: Distributed data processing
- **Delta Lake**: ACID transactions for data lakes
- **AutoLoader**: Incremental file ingestion
- **Unity Catalog**: Data governance and lineage

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ raw_data_ingestion.py
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ data_transformation.py
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ scd_type2_implementation.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ common_functions.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ databricks_asset_bundle.yml
â”‚   â””â”€â”€ pipeline_config.json
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â””â”€â”€ merge_statements.sql
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit_tests.py
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites
- Azure subscription with required services
- Databricks workspace configured
- Unity Catalog enabled
- GitHub repository for CI/CD

### Setup Instructions

1. **Clone Repository**
```bash
git clone https://github.com/hakkache/Spotify-Azure-project.git
cd Spotify-Azure-project
```

2. **Configure Databricks Asset Bundle**
```yaml
bundle:
  name: spotify-data-pipeline
  
resources:
  jobs:
    spotify_etl_job:
      name: "Spotify ETL Pipeline"
      job_clusters:
        - job_cluster_key: "main"
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 2
```

3. **Deploy with Asset Bundles**
```bash
databricks bundle deploy --target dev
databricks bundle run spotify_etl_job --target dev
```

## ğŸ’¾ Data Processing Examples

### Bronze Layer - Raw Ingestion
```python
# AutoLoader for incremental processing
df_stream = spark.readStream \
    .format("cloudFiles") \
    .option("cloudFiles.format", "parquet") \
    .option("cloudFiles.schemaLocation", "/mnt/checkpoints/schema") \
    .load("/mnt/datalake/bronze/")
```

### Silver Layer - Data Transformation
```python
# Clean and validate data
df_cleaned = df_raw \
    .filter(col("user_id").isNotNull()) \
    .withColumn("processed_date", current_timestamp()) \
    .dropDuplicates(["user_id", "stream_date"])
```

### Gold Layer - SCD Type 2 Implementation
```python
# Slowly Changing Dimension Type 2
merge_condition = "target.user_id = source.user_id AND target.is_current = true"

delta_table.merge(
    source_df.alias("source"),
    merge_condition
).whenMatchedUpdate(
    condition = "source.subscription_type != target.subscription_type",
    set = {
        "is_current": "false",
        "end_date": "current_date()"
    }
).whenNotMatchedInsert(
    values = {
        "user_id": "source.user_id",
        "subscription_type": "source.subscription_type",
        "start_date": "current_date()",
        "end_date": "null",
        "is_current": "true"
    }
).execute()
```

## ğŸ”§ Configuration Management

### Unity Catalog Setup
```sql
-- Create catalog and schema
CREATE CATALOG IF NOT EXISTS spotify_catalog;
CREATE SCHEMA IF NOT EXISTS spotify_catalog.processed_data;

-- Grant permissions
GRANT USE CATALOG ON CATALOG spotify_catalog TO `data-engineers`;
GRANT CREATE SCHEMA ON CATALOG spotify_catalog TO `data-engineers`;
```

### Pipeline Configuration
```json
{
  "source_config": {
    "sql_server": "spotify-sql-server.database.windows.net",
    "database": "SpotifyDB",
    "tables": ["dim_user", "dim_artist", "dim_date", "fact_stream"]
  },
  "processing_config": {
    "batch_size": 10000,
    "checkpoint_location": "/mnt/checkpoints/",
    "trigger_interval": "5 minutes"
  }
}
```

## ğŸ“ˆ Monitoring and Optimization

### Performance Metrics
- **Data Freshness**: < 5 minutes latency
- **Processing Throughput**: 1M+ records/hour
- **Error Rate**: < 0.1%
- **Cost Optimization**: Auto-scaling clusters

### Data Quality Checks
```python
# Implement data validation
def validate_data_quality(df):
    null_check = df.filter(col("user_id").isNull()).count()
    duplicate_check = df.count() - df.dropDuplicates().count()
    
    if null_check > 0 or duplicate_check > 0:
        raise Exception("Data quality issues detected")
```

## ğŸ” Security and Governance

### Access Control
- **Unity Catalog**: Fine-grained permissions
- **Azure AD Integration**: Single sign-on
- **Key Vault**: Credential management
- **Network Security**: Private endpoints

### Data Lineage
- Automatic tracking through Unity Catalog
- Column-level lineage for compliance
- Impact analysis for changes

## ğŸš€ Deployment

### CI/CD with GitHub Actions
```yaml
name: Deploy Databricks Asset Bundle
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Deploy Bundle
        run: |
          databricks bundle deploy --target prod
```

## ğŸ“š Documentation

- [Architecture Deep Dive](docs/architecture.md)
- [Data Dictionary](docs/data-dictionary.md)
- [Deployment Guide](docs/deployment.md)
- [Troubleshooting](docs/troubleshooting.md)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Support

For questions and support:
- Create an issue in this repository
- Contact the data engineering team
- Check the troubleshooting guide

---

**Built with â¤ï¸ using Azure Cloud Services and Modern Data Engineering practices**